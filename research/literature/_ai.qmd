```{python}
#| echo: false
from literature.charts.ai import models_chart, scifi_chart, growth_compute_chart, models_rag_performance
import warnings
warnings.filterwarnings("ignore")
```

```{=mdx}
import Figure from '/src/components/Figure'
import AI from '../images/ai/ai-credibility.png'
import Replika from '../images/ai/with-me.png'
import ETF from '../images/ai/etfmatic.png'
import Runaround from '../images/ai/runaround.png'
import ChatGPTMedical from '../images/ai/chatgpt-medical.png'
import AbstractAI from '../images/ai/abstract-ai.png'
```
# AI

::: {.content-hidden unless-format="pdf"}
![Visual abstract for the AI chapter](./images/ai/abstract-ai.png)
:::

```{=mdx}
<Figure caption="Visual abstract for the AI chapter" src={AbstractAI} />
```
## Human Patterns

The fact that AI systems work so well is proof that we live in a measurable world. The world is filled with structures: nature, cultures, languages, human interactions - all form intricate patterns. Computer systems are increasingly capable in their ability copy these patterns into computer models - known as machine learning. As of 2023, 97 zettabytes (and growing) of data was created in the world per year [@soundaryajayaramanHowBigBig2023]. Big data is a basic requirement for training AIs, enabling learning from the structures of the world with increasing accuracy. Large data-sets such as the LAION-5B of 5.85 billion image-text pairs, were foundational for training AI to recognize images [@romainbeaumontLAION5BNEWERA2022; @schuhmannLAION5BOpenLargescale2022]. Just 3 years later, *generating* images with GenAI models is now fast enought to create images in real-time while the user is typing [@dwarkeshpatelMarkZuckerbergLlama2024]. Similarly huge data-sets exist about other types of media - and the open Internet itself, albeit less structured, is a data-source frequently scraped by AI-model builders. Representations of the real world in digital models enable humans to ask questions about the real-world structures and to manipulate them to create synthetic experiments that may match the real world (if the model is accurate enough). This can be used for generating human-sounding language and realistic images, finding mechanisms for novel medicines as well as understanding the fundamental functioning of life on its deep physical and chemical level [@nopriorsInceptiveCEOJakob2023]. Venture capitalists backing OpenAI describe AI as a foundational technology, which will unlock human potential across all fields of human activity [@greylockOpenAICEOSam2022].

In essence, *human patterns* enable AIs. Already 90 years ago [@mccullochLogicalCalculusIdeas1943] proposed the first mathematical model of a neural network inspired by the human brain. Alan Turing's Test for Machine Intelligence followed in 1950. Turing's initial idea was to design a game of imitation to test human-computer interaction using text messages between a human and 2 other participants, one of which was a human, and the other - a computer. The question was, if the human was simultaneously speaking to another human and a machine, could the messages from the machine be clearly distinguished or would they resemble a human being so much, that the person asking questions would be deceived, unable to realize which one is the human and which one is the machine? [@turingCOMPUTINGMACHINERYINTELLIGENCE1950].

> Alan Turing: *"I believe that in about fifty years' time it will be possible to program computers, with a storage capacity of about 10^9^, to make them play the imitation game so well that an average interrogator will not have more than 70 percent chance of making the right identification after five minutes of questioning. ... I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted."* - from [@stanfordencyclopediaofphilosophyTuringTest2021]

By the 2010s AI models became capable enough to beat humans in games of Go and Chess, yet they did not yet pass the Turing test. AI use was limited to specific tasks. While over the years, the field of AI had seen a long process of incremental improvements, developing increasingly advanced models of decision-making, it took an ***increase in computing power*** and an approach called ***deep learning***, a variation of ***machine learning (1980s),*** largely modeled after the ***neural networks*** of the biological (human) brain, returning to the idea of ***biomimicry***, inspired by nature, building a machine to resemble the connections between neurons, but digitally, on layers much deeper than attempted before. Like quantum computing, AI more of a discovery, thank an invention; we have no idea, what are the limits of intelligence [@catgptWhyAIMore2025].

Founder of NVIDIA, Jensen Huang, whose computer chips power much of this revolution, calls it the *"Intelligence Infrastructure"*, produced by intelligence factories, and integrated into everything, just like electricity was [@nvidiaNVIDIACEOJensen2025]. In order to produce this intelligence, huge AI factories are being built around the world, measured in the energy requirements. [@calmaAICouldConsume2025] predicts AI will surpass Bitcoin's energy use by the end of 2025 [@calmaAICouldConsume2025]. The 500B USD Stargate project, is currently building 1.2 gigawatts of AI capacity in the Texas, and expanding to other areas around the U.S., and data center in Abu Dhabi, U.A.E., which requires 5GW of energy, and is physically bigger than the country of Monaco [@loizosOpenAIsPlannedData2025; @mossOpenAICFOStargate2025]. In comparison, the 500MW xAI AI factory, built by Elon Musk's company, powered by natural gas generators, is moderate in size [@wangOpenAIStargatePhase2025]. While OpenAIs Sam Altman is repeatedly quoted as saying the productivity gains created by AI will far offset any of its environmental footprint or other words to that effect [@altmanIntelligenceAge2024; @dipizioSamAltmanSays2023], critics like [@igeniusLetTalkSustainable2020] argue that AI cannot enable a sustainable future if it is not sustainable by design; training and delivery of AI products must include sustainability considerations tied into data intelligence and business analytics.

### Human Feedback

Combining deep learning and *reinforcement learning with human feedback (RLHF)* enabled to achieve levels of intelligence high enough to beat the Turing test [@karamankeChatGPTArchitectBerkeley2022; @christianoMyResearchMethodology2021; @christianoDeepReinforcementLearning2017]. John Schulman, a co-founder of OpenAI describes RLHF simply: *"the models are just trained to produce a single message that gets high approval from a human reader"* [@karamankeChatGPTArchitectBerkeley2022]. Bigger models aren't necessarily better; rather models need human feedback to improve the quality of responses [@ouyangTrainingLanguageModels2022].

The nature-inspired approach was successful. Innovations such as *back-propagation* for reducing errors through updating model weights and *transformers* for tracking relationships in sequential data (for example in sentences), enabled AI models to became increasingly capable [@vaswaniAttentionAllYou2017; @merrittWhatTransformerModel2022]. ***Generative Adversarial Networks*** trained models through pitting them against each other [@goodfellowGenerativeAdversarialNetworks2014]. ***Large Language Models***, enabled increasingly generalized models, capable of more complex tasks, such as language generation [@radford2018improving].

One of the leading scientists in this field of research, Geoffrey Hinton, had attempted back-propagation already in the 1980s and reminiscents how:

> *"the only reason neural networks didn't work in the 1980s was because we didn't have have enough data and we didn't have enough computing power"* [@cbsmorningsFullInterviewGodfather2023].

[@epochaiDataNotableAI2024] reports the growth in computing power and the evolution of more than 800 AI models since the 1950s. Very simply, more data and more computing power means more intelligent models.

```{python}
growth_compute_chart()
```

The above chart shows an illustration of how transformers work by [@alammarIllustratedTransformer2018].

By the 2020s, AI-based models became a mainstay in medical research, drug development, patient care [@LEITE20212515; @holzingerAILifeTrends2023a], quickly finding potential vaccine candidates during the COVID19 pandemic [@ZAFAR2022249], self-driving vehicles, including cars, delivery robots, drones in the sea and air, as well as AI-based assistants. The existence of AI models has wide implications for all human activities from personal to professional. The founder of the largest chimp-maker NVIDIA calls upon all countries do develop their own AI-models which would encode their local knowledge, culture, and language to make sure these are accurately captured [@worldgovernmentssummitConversationFounderNVIDIA2024].

OpenAI has researched a wide range of approaches towards artificial general intelligence (AGI), work which has led to advances in large language models[@ilyasutskeverIlyaSutskeverAI2018; @aifrontiersIlyaSutskeverAI2018]. In 2020 OpenAI released a LLM called GPT-3 trained on 570 GB of text [@alextamkinHowLargeLanguage2021] which was adept in text-generation. [@Singer2022MakeAVideoTG] describes how collecting billions of images with descriptive data (for example the descriptive *alt* text which accompanies images on websites) enabled researchers to train AI models such as ***stable diffusion*** for image-generation based on human-language. These training make use of *Deep Learning*, a layered approach to AI training, where increasing depth of the computer model captures minute details of the world. Much is still to be understood about how deep learning works; even for specialists, the fractal structure of deep learning can only be called *mysterious* [@sohl-dicksteinBoundaryNeuralNetwork2024].

AI responses are probabilistic and need some function for ranking response quality. Achieving higher percentage or correct responses requires oversight which can come in the form of human feedback or by using other AIs systems which are deemed to be already well-aligned (termed Constitutional AI by Anthropic) [@baileyAIEducation2023; @baiConstitutionalAIHarmlessness2022]. One approach to reduce non-alignmnet issues with AI is to introduce some function for human feedback and oversight to automated systems. Human involvement can take the form of interventions from the AI-developer themselves as well as from the end-users of the AI system. Such feedback is not only provided by humans, computer can give feedback to computers too. Less powerful AIs are taught by more poweful and aligned AIs, which understand the world better, to follow human values: for example META used LLAMA 2 for aligning LLAMA 3.

There are many examples of combination of AI and human, also known as *"human-in-the-loop",* used for fields as diverse as training computer vision algorithms for self-driving cars and detection of disinformation in social media posts [@wuHumanintheLoopAIEnhancing2023; @bonet-joverSemiautomaticAnnotationMethodology2023]. Also known as Human-based computation or Human-aided Artificial Intelligence [@Shahaf2007TowardsAT; @muhlhoffHumanaidedArtificialIntelligence2019]. [@gewangHumansLoopDesign2019] from the Stanford Institute for Human-Centered Artificial Intelligence, describes core design principles for building interactive AI systems that augment rather than replace people: (1) value human agency, (2) offer granularity of control, and (3) provide transparency interfaces.

| App | Category | Use Case |
|-------------------|-------------------|----------------------------------|
| Welltory | Health | Health data analysis |
| Wellue | Health | Heart arrhythmia detection |
| QALY | Health | Heart arrhythmia detection |
| Starship Robots | Delivery | The robot may ask for human help in a confusing situation, such as when crossing a difficult road |

Table: Examples of human-in-the-loop apps. []{short-caption="Human-in-the-Loop Apps"}

In order to provide human feedback, systems need to be able to distinguish humans from AIs. To that end, several "Proof of Humanity" toolsets are in the process of being built. [@gitcoinpassport2023] discusses how to build Gitcoin Passport’s Unique Humanity Score, an antifragile passport, inspired by Nassim Taleb's popular book [@talebAntifragileThingsThat2012]. Taleb defines "antifragility" as "systems that benefit from volatility and stressors", summarizing it in a letter to Nature thus:

> "a convex response to a stressor or source of harm (for some range of variation), leading to a positive sensitivity to increase in volatility" - antifragility.

Gitcoin's Passport pulls together proofs of identity from web2 platforms - but adds a unique twist: "Cost of Forgery" as a protection against fake users (aka Sybil attacks, where a malicious person fakes identities so it looks like many independent users), it becomes more expensive for them to do so, turning attack pressure into a self-reinforcing defense; however, while this approach works, it does set a very high bar for users to comply, and requires a cryptocurrency to set the price for the attacks [@gitcoinpassport2023]. In contrast, another popular proof-of-personhood protocol called World, verifies humanness via physical scans of human iris', captured by its Orb device; and again using cryptography, to compare a proof (ZK-SNARK) against a centralized database [@gentCryptocurrencyMassesUniversal2023]. From the user experience perspective, this approach is much simpler (while needing physical presence for the iris scan). Given that World was co-founded by the OpenAI co-founder Sam Altman, this may be one way he plans to counter the possible societal disruptions accelerated by OpenAI's products.

### AI as the *Idiot Savant*

Hinton likes to call AI an *idiot savant*: someone with exceptional aptitude yet serious mental disorder [@cbsmorningsFullInterviewGodfather2023]. Large AI models don't understand the world like humans do. Their responses are predictions based on their training data and complex statistics. Indeed, the comparison is apt, as the AI field now offers jobs for *AI psychologists*, whose role is to figure out what exactly is happening inside the 'AI brain' [@waddellAIMightNeed2018]. Understanding the insides of AI models trained of massive amounts of data is important because they are *foundational*, enabling a holistic approach to learning, combining many disciplines using languages, instead of the reductionist way we as human think because of our limitations [@capinstituteGettingRealArtificial2023]. Hinton received a Nobel Prize for modeling how the brain works and coming up with the idea of predicting the next word in a sequence, already in 1986, which later became the basis for large language models [@cbsmorningsAIPioneerGeoffrey2025].

Foundation models enable *Generative AIs*, a class of models which are able to generate many types of \*tokens\*\*, such as text, speech, audio [@sanroman2023fromdi; @kreukAudioGenTextuallyGuided2022], music [@copetSimpleControllableMusic2023; @metaaiAudioCraftSimpleOnestop2023], video, and even complex structures such 3D models and DNA structures, in any language it's trained on. The advent of generative AIs was a revolution in human-computer interaction as AI models became increasingly capable of producing human-like content which is hard to distinguish from actual human creations. This power comes with *increased need for responsibility*, drawing growing interest in fields like *AI ethics* and *AI explainability.* Generative has a potential for misuse, as humans are increasingly confused by what is computer-generated and what is human-created, unable to separate one from the other with certainty.

[@bommasaniOpportunitiesRisksFoundation2021] define *foundation models* as large scale pretrained models adaptable to diverse downstream tasks, thoroughly accounting opportunities, such as capabilities across language, vision, robotics and reasoning - and risks: bias, environmental cost, economic shifts, governance, highlighting the need for interdisciplinary research - to understand deeply how these models work, and when and how do they fail. Understanding failure is crucial, as there is the question of who bares the responsibility for the actions taken by the AI (especially, in its most agentic forms, with access to the internet and tools outside the model itself). Research in organizational behavior indicates that when individuals exert influence through intermediaries - known as *indirect agency*, - their ethical judgment can become distorted: humans may believe they are behaving ethically while, in reality, they exhibit reduced concern for those affected by their decisions, resulting in less accountability for moral failures, and expecting fewer consequences for unethical conduct [@gratchPowerHarmAI2022].

The technological leap is disruptive enough for people to start calling it the start of a new era. [@nobleFifthIndustrialRevolution2022] proposes AI has reached a stage of development marking beginning of the *5th industrial revolution*, a time of collaboration between humans and AI. Widespread Internet of Things (IoT) sensor networks that gather data analyzed by AI algorithms, integrates computing even deeper into the fabric of daily human existence. Several terms of different origin but considerable overlap describe this phenomenon, including *Pervasive Computing (PC)* [@rogersFourPhasesPervasive2022] and *Ubiquitous Computing*. Similar concepts are *Ambient Computing*, which focuses more on the invisibility of technology, fading into the background, without us, humans, even noticing it, and *Calm Technology*, which highlights how technology respects humans and our limited attention spans, and doesn't call attention to itself. In all cases, AI is integral part of our everyday life, inside everything and everywhere. Today AI is not an academic concept but a mainstream reality, affecting our daily lives everywhere, even when we don't notice it.

### Algorithmic Experience and Transparency: Before AIs

Before AIs, as a user of social media, one may be accustomed to interacting with the feed algorithms that provide a personalized *algorithmic experience*. Social media user feed algorithms are more *deterministic* than AI, meaning they would produce more predictable output in comparison AI models. Nonetheless, there are many reports about effects these algorithms have on human psychology, including loneliness, anxiety, fear of missing out, social comparison, and even depression [@deSocialMediaAlgorithms2025; @qiuPsychiatristsPerspectiveSocial2021].

Design is increasingly relevant to algorithms, - *algorithm design* - and more specifically to algorithms that affect user experience and user interfaces. ***When the design is concerned with the ethical, environmental, socioeconomic, resource-saving, and participatory aspects of human-machine interactions and aims to affect technology in a more human direction, it can hope to create an experience designed for sustainability.***

[@lorenzoDaisyGinsbergImagines2015] underlines the role of design beyond *designing* as a tool for envisioning; in her words, *"design can set agendas and not necessarily be in service, but be used to find ways to explore our world and how we want it to be"*. Practitioners of Participatory Design (PD) have for decades advocated for designers to become more activist through ***action research***. This means to influencing outcomes, not only being a passive observer of phenomena as a researcher, or only focusing on usability as a designer, without taking into account the wider context.

[@shenoiParticipatoryDesignFuture2018] argues inviting domain expertise into the discussion while having a sustainable design process enables designers to design for experiences where they are not a domain expert; this applies to highly technical fields, such as medicine, education, governance, and in our case here - finance and sustainability -, while building respectful dialogue through participatory design. After many years of political outcry [@crainPoliticalManipulationInternet2019], social media platforms such Meta Facebook and Twitter (later renamed to X) have began to shed more light on how these algorithms work, in some cases releasing the source code [@nickcleggHowAIInfluences2023; @twitterTwitterRecommendationAlgorithm2023].

The content on the platform can be more important than the interface. Applications with a similar UI depend on the community as well as the content and how the content is shown to the user.

### Transitioning to Complexity: Non-Deterministic Systems

AIs are non-deterministic, which requires a new set of consideration when designing AI. AI systems may make use of several algorithms within one larger model. It follows that AI Explainability requires *Algorithmic Transparency*.

### Being Responsible, Explainable, and Safe: Legislation Adapts and Sets Boundaries for AI

On March 13, 2024, the European Parliament (with 523 votes for and 46 against) the EU AI Law, taking a risk-based approach to a regulatory framework, which aims to support innovation, while safeguarding democracy and environmental sustainability [@lomasDealEUAI2024]. Specifically, the EU Artificial Intelligence Act (Regulation EU 2024/1689) establishes the first comprehensive legal framework for AI in the world, aiming to harmonize rules to ensure that AI systems are safe, human-centric, and rights-respecting; the act defines a tiered system that bans unacceptable risks and regulates high-risk uses, imposing transparency duties on developers of AI systems, for example near-real-time (hourly) CO~2~eq emissions reports from the AI models [@EUAIAct2024]. As AI-based solutions permeate every aspect of human life, legislation is starting to catch up. In order to help international jurisdictions tailor which incidents and hazards they track and enable interoperability, the Organization for Economic Cooperation and Development (OECD) later also defined 2 types of AI risk, "AI incident" - AI system causes real harm; "AI hazard" - potential‐harm scenario, both which can be raised to "serious" variants [@oecdDefiningAIIncidents2024].

*"As humans we tend to fear what we don't understand"* is a common sentiment which has been confirmed psychology [@allportNaturePrejudice1979]. Current AI-models are opaque '*black boxes'*, where it's difficult to pin-point exactly why a certain decision was made or how a certain expression was reached, not unlike inside the human brain. This line of thought leads me to the idea of ***AI Psychologists,*** who might figure out the ***Thought Patterns*** inside the model. Research in AI-explainability (XAI in literature) is on the lookout for ways to create more ***Transparency and Credibility*** in AI systems, which could lead to building trust in AI systems and would form the foundations for ***AI Acceptance***.

The problems of opaqueness create the field of *Explainable AI*. [@bowmanEightThingsKnow2023] says steering Large Language Models is unreliable; even experts don't fully understand the inner workings of the models. Work towards improving both ***AI steerability*** and ***AI Alignment*** (doing what humans expect) is ongoing. [@holbrookHumanCenteredMachineLearning2018] argues that in order to reduce errors which only humans can detect, and provide a way to stop automation from going in the wrong direction, it's important to focus on making users feel in control of the technology. There's an increasing number of tools for LLM evaluation. "Evaluate and Track LLM Applications, Explainability for Neural Networks" [@trueraTruLens2023; @leinoInfluenceDirectedExplanationsDeep2018]. [@liangHolisticEvaluationLanguage2022] believes there's early evidence it's possible to assess the quality of LLM output transparently. [@CABITZA2023118888] proposes a framework for explainability of AI-expressions to guide XAI research, focusing on the quality of formal soundness and cognitive clarity. [@khosraviExplainableArtificialIntelligence2022] proposes a framework for AI explainability, focused squarely on education, which brings in communication with stakeholders and human-centered interface design [@holzingerMultimodalCausabilityGraph2021] highlights possible approaches to implementing transparency and explainability in AI models, introducing the concept of *multimodal causality*, where an AI system uses pictures, text, and charts all at once, which could help the human user see cause and effect across different kinds of data.

The chart below displays the AI Credibility Heuristics: A Systematic Model, which explains how (similarly to Daniel Kahneman's book "Thinking, Fast and Slow"), AI...

::: {.content-hidden unless-format="pdf"}
![Heuristic-Systematic Model of AI Credibility](./images/ai/ai-credibility.png){width="100%"}
:::

```{=mdx}
<Figure caption="Heuristic-Systematic Model of AI Credibility" src={AI} />
```
A movement called *Responsible AI* seeks to mitigate generative AIs' known issues. Given the widespread use of AI and its increasing power of foundational models, it's important these systems are created in a safe and responsible manner. While there have been calls to pause the development of large AI experiments [@futureoflifeinstitutePauseGiantAI2023] so the world could catch up, this is unlikely to happen. There are several problems with the current generation of LLMs from OpenAI, Microsoft, Google, Nvidia, and others.

[@christianoMyViewsDoom2023] believes there are plenty of ways for bad outcomes (existential risk) even without extinction risk. In order to mitigate these risks (and perhaps to appease the public), all the major AI labs have taken steps to be more safe. Anthropic, which was founded by former OpenAI employees, after leaving the OpenAI over this very issue, led the movement by announcing responsible *scaling policy* [@anthropicAnthropicsResponsibleScaling2023]. OpenAI itself announced a dedicated "Superalignment" team, co-led by Ilya Sutskever and Jan Leike; they made a specific promise to commit 20% of its compute budget to build an AI system in the next 4 years, that can itself research and refine alignment methods, effectively solving the alignment problem for superintelligent AI (which is considered the highest risk) [@janleikeIntroducingSuperalignment2023]. OpenAI has previously admitted, it does not yet fully understand how the internals of an neural network work; they are developing tools to represent neural network concepts for humans [@openaiExtractingConceptsGPT42024; @gaoScalingEvaluatingSparse2024]. Outside the major labs, several independent AI safety organizations have also been launched, for example METR, the Model Evaluation & Threat Research incubated in the Alignment Research Center [@METR2023].

A popular approach to AI safety is *red-teaming*, which means pushing the limits of LLMs, trying to get them to produce outputs that are racist, false, or otherwise unhelpful. Mapping the emerging abilities of new models is a job in itself.

|  |  |
|--------------------------|----------------------------------------------|
| **Problem** | **Description** |
| Monolithicity | LLMs are massive monolithic models requiring large amounts of computing power for training to offer ***multi-modal*** ***capabilities*** across diverse domains of knowledge, making training such models possible for very few companies. @liuPrismerVisionLanguageModel2023 proposes future AI models may instead consist of a number networked domain-specific models to increase efficiency and thus become more scalable. |
| Opaqueness | LLMs are opaque, making it difficult to explain why a certain prediction was made by the AI model. One visible expression of this problem are ***hallucinations**,* the language models are able to generate text that is confident and eloquent yet entirely wrong. Jack Krawczyk, the product lead for Google's Bard (now renamed to Gemini): "Bard and ChatGPT are large language models, not knowledge models. They are great at generating human-sounding text, they are not good at ensuring their text is fact-based. Why do we think the big first application should be Search, which at its heart is about finding true information?" |
| Biases and Prejudices | AI bias is well-documented and a hard problem to solve [@liangGPTDetectorsAre2023]. **Humans don't necessarily correct mistakes made by computers and may instead become "partners in crime"** [@krugelAlgorithmsPartnersCrime2023]. People are prone to bias and prejudice. It's a part of the human psyche. Human brains are limited and actively avoid learning to save energy. These same biases are likely to appear in LLM outputs as they are trained on human-produced content. Unless there is active work to try to counter and eliminate these biases from LLM output, they will appear frequently. |
| Missing Data | LLMs have been pre-trained on massive amounts of public data, which gives them the ability for reasoning and generating in a human-like way, yet they are missing specific private data, which needs to be ingested to augment LLMs ability to respond to questions on niche topics [@Liu_LlamaIndex_2022]. |
| Data Contamination | Concerns with the math ability of LLMs. "Performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability" @zhangCarefulExaminationLarge2024 |
| Lack of Legislation | @anderljungFrontierAIRegulation2023 OpenAI proposes we need to proactively work on common standards and legislation to ensure AI safety. It's difficult to come up with clear legislation; the U.K. government organized the first AI safety summit in 2023 @browneBritainHostWorld2023. |

Table: Summarizing some problems with contemporary AIs. []{short-caption="Problems with contemporary AIs"}

In 2024, OpenAI released its *"Model Spec"* to define clearly their approach to AI safety with the stated intention to provide clear guidelines for the RLHF approach [@openaiIntroducingModelSpec2024].

### Evolution of Models and Emerging Abilities

The debate between open source vs closed-source AI is ongoing. Historically, open-source has been useful for finding bugs in code as more pairs of eyes are looking at the code and someone may see a problem the programmers have not noticed. Proponents of closed-source development however worry about the dangers or releasing such powerful technology openly and the possibility of bad actors such as terrorists, hackers, violent governments using LLMs for malice. Whether closed-sourced or open-sourced development will be lead to more AI safety is one of the large debates in the AI industry.

Personal AI assistants to date have been created by large tech companies, mostly using closed-source AI. However, open-source AI-models have opened up the avenue for smaller companies and even individuals for creating new AI-assistants - perhaps using the same underlying foundation model as the base, but adding new data, abilities, tools, or just innovating on the UI/UX stack. An explosion of personal AI assistants powered by foundation models can be found across use-cases. The following table only lists a tiny sample of such products.

| App          | Features                 |
|--------------|--------------------------|
| socratic.org | Study buddy              |
| youper.ai    | Mental health helper     |
| fireflies.ai | Video call transcription |
| murf.ai      | Voice generator          |

Table: AI-based Assistants []{short-caption="AI-based Assistants"}

In any case, open or closed-sourced, real-world usage of LLMs may demonstrate the limitations and edge-cases of AI. Hackathons such as [@peteWeHostedEmergencychatgpthackathon2023] help come up with new use-cases and disprove some potential ideas. The strongest proponent of Open Source AI, META, open-sourced the largest language model (70 billion parameters) which with performance rivaling several of the proprietary models; because META's core business is not AI, rather it would benefit from having access to cheaper, better AI across the board, open-sourcing may be their best strategy [@dwarkeshpatelMarkZuckerbergLlama2024].

| AI Model | Released | Company | License | Country |
|---------------|---------------|---------------|---------------|---------------|
| GPT-1 | 2018 | OpenAI | Open Source | U.S. |
| GTP-2 | 2019 | OpenAI | Open Source | U.S. |
| Turing-NLG | 2020 | Microsoft | Proprietary | U.S. |
| GPT-3 | 2020 | OpenAI | Open Source | U.S. |
| GPT-3.5 | 2022 | OpenAI | Proprietary | U.S. |
| GPT-4 | 2023 | OpenAI | Proprietary | U.S. |
| AlexaTM | 2022 | Amazon | Proprietary | U.S. |
| NeMo | 2022 | NVIDIA | Open Source | U.S. |
| PaLM | 2022 | Google | Proprietary | U.S. |
| LaMDA | 2022 | Google | Proprietary | U.S. |
| GLaM | 2022 | Google | Proprietary | U.S. |
| BLOOM | 2022 | Hugging Face | Open Source | U.S. |
| Falcon | 2023 | Technology Innovation Institute | Open Source | U.A.E. |
| Tongyi | 2023 | Alibaba | Proprietary | China |
| Vicuna | 2023 | Sapling | Open Source | U.S. |
| Wu Dao 3 | 2023 | BAAI | Open Source | China |
| LLAMA 2 | 2023 | META | Open Source | U.S. |
| PaLM-2 | 2023 | Google | Proprietary | U.S. |
| Claude 3 | 2024 | Anthropic | Proprietary | U.S. |
| Mistral Large | 2024 | Mistral | Proprietary | France |
| Gemini 1.5 | 2024 | Google | Proprietary | U.S. |
| LLAMA 3 | 2024 | META | Open Source | U.S. |
| AFM | 2024 | Apple | Proprietary | U.S. |
| Viking 7B | 2024 | Silo | Open Source | Finland |
| GPT-4.5 | 2025 | OpenAI | Proprietary | U.S. |
| DeepSeek-R1 | 2025 | Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd 杭州深度求索人工智慧基礎技術研究有限公司 | Open Source | China |
| GPT-5 | 202? | OpenAI | Unknown; trademark registered | U.S. |

Table: Summary of 7 years of rapid AI model innovation since the first LLM was publicly made available in 2018 [@brown2020language; @tamkin2021; @alvarezGenerateChatbotTraining2021; @hinesOpenAIFilesTrademark2023; @metaIntroducingMetaLlama2024; @baptistaDeepSeekRushesLaunch2025; @deepseek-aiDeepSeekR1IncentivizingReasoning2025]. []{short-caption="7 Years of Rapid AI Model Innovation"}

```{python}
models_chart()
```

A foundational paper on the scaling laws of LLMs by [@kaplanScalingLawsNeural2020] provided a quantitative road-map linking model, data, and compute to predict performance; helpful to guide large-scale investment into LLMs. The proliferation of different models enables comparisons of performance based on several metrics from accuracy of responses to standardized tests such as GMAT usually taken my humans to reasoning about less well-defined problem spaces. [@chiang2024chatbot; @lmsys.orgGPT4TurboHasJust2024] open-source AI-leaderboard project has collected over 500 thousand human-ranking of outputs from 82 large-language models, evaluating reasoning capabilities, which as of 2024 rate GPT-4 and Claude 3 Opus as the top-performers. Model performance is not one-dimensional; [@openaiHelloGPT4o2024] show how GPT 4o combines different abilities into the same model, preserving more information, which in previous models was lost in data conversion (for example for images). Another metric is metacognition, defined as *knowing about knowing* [@metcalfeMetacognitionKnowingKnowing1994] or "*keeping track of your own learning"* as defined by educators in sustainability (an example of how the same term is useful across academic fields) [@zerowasteeuropeZeroWasteHandbook2022]. Anthropic's Claude 3 was the first model capable of metacognition, promoting it as a feature, calling out a mistake made by itself [@shibuModelOpenAIRival2024].

With the proliferation of AI models, AI benchmarking has developed into its own industry, with many ways to measure a model's performance. In the early days [@hendrycksMeasuringMassiveMultitask2020] revealed models' uneven knowledge and lack in calibration, with the introduction of MMLU (Measuring Massive Multitask Language Understanding), a 57-task benchmark covering domains from elementary math to law, showing GPT-3 43.9% accuracy vs 89.8% human experts (19 points above random chance but far below human-expert level). Later models have reached or surpassed humans in this particular benchmark, necessitating the creation of newer, more difficult tests for AI. Anoter foundational AI paper, [@zellersHellaSwagCanMachine2019]'s HellaSwag, is also accompanied by a leaderboard website (still being updated after publication) listing AI model performance most recent entry April 16, 2024.

Moreover, benchmarking is not only about the abilities, knowledge and alignment of the model itself. Interactions with other systems are equally important to measure, such as Retrieval Augmented Generation (RAG) performance. RAG is used to enhance AI content with domain-specific (close-to real-time) knowledge. A technique first proposed by researchers at META [@lewisRetrievalAugmentedGenerationKnowledgeIntensive2020] some RAG benefits include *"make contextual decisions on-the-fly, thereby opening up a more dynamic and responsive way to handle knowledge search tasks"* [@dewyBuildingRAGTool2024; @gaoRetrievalAugmentedGenerationLarge2023]. Generative AI applications retrieve data from unstructured external sources in order to augment LLMs existing knowledge with current information [@lengLongContextRAG2024]. [@ragasMetricsDrivenDevelopment2023] suggests evaluating one's RAG pipelines enables *Metrics-Driven Development*. Likewise, LangSmith, the developer platform for LLM-powered apps (which makes extensive use of RAG), dissects the LLM app lifecycle into a pipeline: debug, collaborate, test, and monitor [@langchainDynamicFewshotExamples2024]. As using unstructured inputs to generated structured data, is one of the core use cases of LLMs, conforming the outputs strictly to standards such as JSON is crucial (otherwise the production app might even break), explains why OpenAI’s Structured Outputs, which guaranteed 100% reliability, was an important jump in AI adoption to mainstream app development [@pokrassIntroducingStructuredOutputs2024].

```{python}
models_rag_performance()
```

Meta's head AI researcher Yann LeCun predicts LLMs may have reached their limitations, for innovation AIs need to understand the physical world and do reasoning in abstract space, which does not require a language, i.e. something a cat could do when figuring out where to jump; in comparison, languages are simple because they are discrete, with very little noise [@nvidiadeveloperFrontiersAIComputing2025].

### Price of Tokens vs Price of Human Labor

At the end of the day, the adoption of AI to everyday life, even in the smallest of contexts, will come down to the price. Long-time AI-engineer [@ngAIRestoresALS2024] predicts, having seen the roadmaps for the microchip industries, as well as incoming hardware and software innovations, the price of tokens will be very low, and much lower than a comparative human worker.

## Human Acceptance of Artificial Companions

### Human Expectations Take Time to Change

*AI acceptance* is incumbent on traits that are increasingly human-like and would make a human be acceptable: credibility, trustworthiness, reliability, dependability, integrity, character, etc. [@ZHANG2023107536] found humans are more likely to trust an AI teammate if they are not deceived by its identity. It's better for collaboration to make it clear, one is talking to a machine. One step towards trust is the explainability of AI-systems. AIs should disclose they are AIs.

[@zerilliHowTransparencyModulates2022] focuses on human factors and ergonomics and argues that transparency should be task-specific: while transparency is key to trust and system monitoring, it should extend beyond explainability; after AI makes an error, different forms of AI transparency: (1) explanations, (2) confidence metrics, (3) human control over *task allocation* - affect human confidence in the system and have diverse levels of ability to repair human trust in the AI. To expand on the third point discussed by this author, in *adaptable allocation*, the user always decides when to keep a task and when to hand it to the AI algorithm - and in *adaptive allocation*, the system decides itself (by monitoring its own uncertainty) when to give difficult or risky cases back to the human.

Humans still need some time to adjust their expectations of *what's possible* using conversational AI interfaces. [@baileyAIEducation2023] believes people are used to *search engines*, and it will take a little bit of time to get familiar with talking to a computer in natural language to accomplish their tasks. For example, new users of v0, an AI assistant for building user interfaces through conversation, would tell humans (the company make this app) about the issues they encounter, instead of telling the AI assistant directly, even though the AI in many cases would be able to fix the problem instantly; human users don't yet necessarily expect computers to behave like another human, there's *inertia* in the mental model of what computers are capable of, requiring the user interfaces to provide context and teaching humans how to interact with their AI coworkers[@rauchFascinatingFindingV02024]. Indeed, ChatGPT is already using buttons to explain context [@feifeiliuLiuFeiFeiPromptControlsGenAI].

Speaking in the mother language of the users is a way to gain trust. English is still over-represented in current models so some local models focus on better understanding local context, such as the Finnish [@SiloAINew2024] focuses on Nordic languages. However, as time progresses, large, general-purpose LLMs may catch up and integrate all this knowledge - or even potentially being taught by the local models.

### Affective Computing: Towards Friendly Machines

*Rosalind Picard* founded the field of *affective computing*, aiming to make computers more human-friendly, pioneering early approaches to recognizing human emotions with sensors and providing users experiences that take human emotion into account [@picardAffectiveComputing1997].

It's not an overstatement to say that data from all the surrounding processes will define the future of computing [@hiittvWojciechSzpankowskiEmerging2021]. In the early examples, electrodermal activity of the skin and heart-rate variance data were used to detect the emotional state and stress level of the user [@zangronizElectrodermalActivitySensor2017; @velmovitskyUsingAppleWatch2022]. This technology has since become mainstream in products such as Fitbit and the Apple Watch, among many others.

*Personal experience:*

> Apple Watch features Fall Detection, which I've experienced personally. Riding my bicycle to the NCKU library after rain, I slipped and landed on my stomach on the pavement. Apple Watch immediately asked me: *"It looks like you've taken a hard fall"* and offered an option to call the ambulance. Fortunately I was OK but if I did need assistance, this AI algorithm delivered contextual help, which could save my health!

Affective computing doesn't only rely on sensors. It makes use of human language using the tools of computational linguistics. Such as natural language processing (NLP). Since the first mainframe computers with rudimentary ability to use pattern matching to respond with simple text messages, humans have been drawn to discussions of our private lives with machines. If programmed to be helpful, computers don't necessarily judge us like a human could. A famous anecdote is about the MIT ELIZA project (an early example of a language model) developed by Joseph Weizenbaum in the mid-1960s, where the lab assistant Pamela McCorduck would dedicate extended time to talking to the machine in private. The machine was called ELIZA and would assume the persona of called DOCTOR to emulate a Rogerian psychotherapist, a style of person-centered therapy developed by Carl Rogers, from the core idea that positive psychological functioning is a inherently human motivation [@mccorduckMachinesWhoThink2004; @bassettComputationalTherapeuticExploring2019; @rogersWayBeing1995].

An excerpt from the classic, first-of-a-kind conversation between a human and a machine (called DOCTOR) in 1965, as documented by [@tarnoffWeizenbaumNightmaresHow2023; @weizenbaumELIZAComputerProgram1966]

> User: Well, my boyfriend made me come here.\
> DOCTOR: YOUR BOYFRIEND MADE YOU COME HERE?\
> User: He says I’m depressed much of the time.\
> DOCTOR: I AM SORRY TO HEAR YOU ARE DEPRESSED.\
> User: It’s true. I am unhappy.\
> DOCTOR: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY?\
> User: I need some help, that much seems certain.\
> DOCTOR: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP?\
> User: Perhaps I could learn to get along with my mother.\
> DOCTOR: TELL ME MORE ABOUT YOUR FAMILY.\
> User: My mother takes care of me.\
> DOCTOR: WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?\
> User: My father.\
> DOCTOR: YOUR FATHER?\
> User: You are like my father in some ways.\
> DOCTOR: WHAT RESEMBLANCE DO YOU SEE?

Weizenbaum later expressed concerns about how easily humans might be misled by AIs, by projecting fantasies onto computer systems, cautioning technologists not to neglect human responsibility from societal problems; AI is *not* a universal solution [@z.m.lComputersEnableFantasies2023]

### Artificial Empathy Also Builds Trust

Today's machines are much more capable so it's not a surprise humans would like to talk to them. One example is a conversational chatbot - or *AI Friend* -, called Replika, a computer model trained to be your companion in daily life. Replika was launched in 2017 and in 2024 was used by 30 million people; the focus is on empathetic dialogue to support mental well-being, sort of like a friend, a digital companion, (or even a romantic partner, in paid versions of the app), and includes an animated avatar interface [@eugeniakuydaReplika2023]. Replika can ask probing questions, tell jokes, and learning about your personality and preferences to generate more natural-sounding conversations.[@tristangreeneConfusedReplikaAI2022; @bardhanMenAreCreating2022] report on anecdotal evidence from Reddit boards which shows how some users of the Replika AI companion app feel so much empathy towards the robot, they confuse it with a sentient being, while others are using verbal abuse and gendered slurs to fight with their AI companions. When the quality of AI responses becomes good enough, people begin to get confused. [@jiangChatbotEmergencyExist2022] describes how Replika users in China using in 5 main ways, all of which rely on empathy. The company's CEO insists it's not trying to replace human relationship but to create an entirely new relationship category with the AI companion; there's value for the users in more realistic avatars, integrating the experience further into users' daily lives through various activities and interactions [@patelReplikaCEOEugenia2024].

| How humans express empathy towards the Replika AI companion |
|-------------------------------------------------------------|
| Companion buddy                                             |
| Responsive diary                                            |
| Emotion-handling program                                    |
| Electronic pet                                              |
| Tool for venting                                            |

Table: Replika AI users approach to interacting with the AI friend from @jiangChatbotEmergencyExist2022. []{short-caption="Replika AI Users and AI Friends"}

Surprisingly, humans can have emotionally deep conversations with robots. Jakob Nielsen notes two recent studies suggesting human deem AI-generated responses *more empathetic than human responses,* at times by a significant margin; however telling users the response is AI-generated reduces the perceived empathy [@nielsenUXRoundupAI2024; @ayersComparingPhysicianArtificial2023; @yinAICanHelp2024]. LLMs combined with voice, such as the Pi iOS app, provide a user experience, which [@ethanmollick2023] calls *unnerving*. The company provides *emotional intelligence* as a service and has developed its own proprietary LLM, called Inflection AI, which has raised over 1B USD in funding [@mittalInflection25PowerhouseLLM2024]. While startups are moving fast, traditional AI companies, with decades of AI experience, such as Google, are also developing an AI assistant for giving life advice [@goswamiGoogleReportedlyBuilding2023]. The conversations can be topic-specific. For instance, [@unleashSebastianAi2017] used BJ Fogg's *tiny habits model* to develop a sustainability-focused AI assistant at the Danish hackathon series Unleash, to encourage behavioral changes towards maintaining an aspirational lifestyle, nudged by a chatbot buddy.

On the output side, [@lvCutenessIrresistibleImpact2022] studies the effect of *cuteness* of AI apps on users and found high perceived cuteness correlated with higher willingness to use the apps, especially for emotional tasks. Part of this is learning how to use emojis in the right amount and at the right time; increasingly, emojis are a part of natural human language [@tayWhyScienceNeeds2023].

Already more than two decades ago, [@reevesMediaEquationHow1998] argued that humans expect computers to be like social actors, (not unlike humans or places), with very minimal cues from a machine (like a voice or screen avatar) triggering social behaviors.

### Conversation: A Magical Starting Point of a Relationship

High quality conversations are somewhat magical in that they can establish trust and build rapport which humans. [@CELINO2020102410] found in testing chatbots for survey interfaces that "\[c\]onversational survey lead to an improved response data quality."

There are noticeable differences in the quality of the LLM output, which increases with model size. [@levesqueWinograd2012] developed the *Winograd Schema Challenge*, looking to improve on the Turing test, by requiring the AI to display an understanding of language and context. The test consists of a story and a question, which has a different meaning as the context changes: "The trophy would not fit in the brown suitcase because it was too big" - what does the *it* refers to? Humans are able to understand this from context while a computer models would fail. Even GPT-3 still failed the test, but later LLMs have been able to solve this test correctly (90% accuracy) @kocijanDefeatWinogradSchema2022. This is to say AI is in constant development and improving it's ability to make sense of language.

*ChatGPT* is the first *user interface (UI)* built on top of GPT-4 by OpenAI and is able to communicate in a human-like way - using first-person, making coherent sentences that sound plausible, and even - confident and convincing. @wangEconomicCaseGenerative2023 ChatGPT reached 1 million users in 5 days and 6 months after launch has 230 million monthly active users. While it was the first, competing offers from Google (Gemini), Anthrophic (Claude), Meta (Llama) and others quickly followed starting a race for best performance across specific tasks including standardized tests from math to science to general knowledge and reasoning abilities.

OpenAI provides AI-as-a-service through its *application programming interfaces (APIs),* allowing 3rd party developers to build custom UIs to serve the specific needs of their customer. For example Snapchat has created a *virtual friend* called "My AI" who lives inside the chat section of the Snapchat app and helps people write faster with predictive text completion and answering questions. The APIs make state-of-the-art AI models easy to use without needing much technical knowledge. Teams at AI-hackathons have produced interfaces for problems as diverse as humanitarian crises' communication, briefing generation, code-completion, and many others. While models are powerful, they still need access to other services and tools to be able to achieve the tasks, which humans do online on a daily basis; for this to be possible, the Model Context Protocol (MCP) standard provides the structure to link models to APIs in other services, especially useful in agentic workflows, where the model uses chain-of-thought reasoning and may call various other tools and services in the process [@heidelMCPReasoningMultiple2025; @hungerfordModelcontextprotocolServersModel2025; @pandeyIntroducingAWSServerless2025].

ChatGPT makes it possible to *evaluate AI models* just by talking, i.e. having conversations with the machine and judging the output with some sort of structured content analysis tools. [@oconnorOpenArtificialIntelligence2023 and @cahanConversationChatGPTRole2023] have conversations about science with AI. [@pavlikCollaboratingChatGPTConsidering2023 and @brenta.andersWhyChatGPTSuch2022] report on AI in education. Just as humans, AIs are continuously learning. [@ramchurnTrustworthyHumanAIPartnerships2021] discusses positive feedback loops in continually learning AI systems which adapt to human needs. [@kechtQuantifyingChatbotsAbility2023] suggests AI is even capable of learning business processes.

### Multi-Modality: Natural Interactions with AI Systems, Agents and the Intention Economy

While AI outperforms humans on many tasks, humans are experts in multi-modal thinking, bridging diverse fields. Humans are multi-modal creatures by birth. To varied ability, we speak, see, listen using our biological bodies. AIs are becoming multi-modal by design to be able to match all the human modes of communication - increasing their humanity.

Multimodal model development is ongoing. Previously, providing multi-modal features meant combining several AI models within the same interface. For example, on the input side, one model is used for human speech or image recognition which are transcribed into tokens that can be ingested into an LLM. On the output side, the LLM can generate instructions which are fed into an image / audio generation model or even computer code which can be run on a virtual machine and then the output displayed inside the conversation. However, this is changing, with a single model able to handle several tasks internally (thus losing less data and context). By early 2024, widely available LLMs front-ends such as Gemini, Claude and ChatGPT have all released basic features for multi-modal communication. In the case of Google's Gemini 1.5 Pro, one model is able to handle several types of prompts from text to images. Multimodal prompting however requires larger context windows, as of writing, limited to 1 million tokens in a private version allows combining text and images in the question directed to the AI, used to reason in examples such as a 44-minute Buster Keaton silent film or Apollo 11 launch transcript (404 pages) [@googleMultimodalPrompting44minute2024].

[@fuLearningConversationalAI2022] provides an overview of conversational AI, from a survey of over 100 peer-reviewed articles published 2018-2021 (a long time ago in terms of AI development), categorizing systems into (1) rule-based, (2) retrieval-based, and (3) generative types; generative transformer models have led the AI field, yet continue to face challenges with coherence over extended interactions and ensuring factual accuracy (hallucinations), retrieval-augmented tooling improves information accuracy, and reinforcement learning and fine-tuning approaches are effective in adjusting conversational style and safety; the authors also highlight that human evaluation for reinforcement learning is still required, as commonly used automated evaluation metrics for AI models, such as BLEU, ROUGE, and BERTScore have limited correlation with human judgments.

| Paper Focus Area | Key Insight | Strengths | Limitations |
|-----------------|--------------------|-----------------|------------------|
| Generative transformer models (GenAI) | Recent advancement in AI models | High language fluency, adaptability | Poor long-term coherence, struggles with facts |
| Retrieval-augmented hybrids (RAG) | Retrieval methods enhance truthfulness | Improved factual grounding | Difficulty in integrating retrieved content |
| Reinforcement-learning | Fine-tuning can steer conversational style and safety | Flexible style and safety alignment | High resource usage, sensitive to reward design |

Table: Three areas of focus in conversational AI development in from [@fuLearningConversationalAI2022]. []{short-caption="Areas of Focus in Conversational AI Development"}

Literature also delves into human-AI interactions on almost human-like level discussing what kind of roles can the AIs take. [@seeberMachinesTeammatesResearch2020] proposes a future research agenda for regarding *AI assistants as teammates* rather than just tools and the implications of such mindset shift. From assistant -\> teammate -\> companion -\> friend The best help for anxiety is a friend. AIs are able to assume different roles based on user requirements and usage context. This makes AI-generated content flexible and malleable. The path from \*\*Assistance\* to *Collaboration* requires another level of trust. It's not only what role the AI takes but how that affects the human. As humans have ample experience relating to other humans and as such the approach towards assistants vs a teammate will vary. While [@lenharoChatGPTGivesExtra2023] experimental study reports AI productivity gains, with DALL-E and ChatGPT being qualitatively better than former automation systems, we might still be 1–3 years away from systems that qualify as team-mates. Once AI reaches that level, would it change how do humans treat it? Not because the AI might be hurt, but because how it affects the psyche of the user: this is an area which needs much more attention. One researcher in this field @karpusAlgorithmExploitationHumans2021 is concerned with humans treating AI badly and coins the term *algorithm exploitation*.

*Context of Use,* Where is the AI used? [@schoonderwoerdHumancenteredXAIDeveloping2021] focuses on human-centered design of AI-apps and multi-modal information display. It's important to understand the domain where the AI is deployed in order to develop explanations. However, in the real world, how feasible is it to have control over the domain? @calistoIntroductionHumancentricAI2021 discusses **multi-modal AI-assistant** for breast cancer classification.

If we see the AI as being in human service. [@davidjohnstonSmartAgentProtocol2023] proposes *Smart Agents*, "general purpose AI that acts according to the goals of an individual human". AI agents can enable *Intention Economy* where one simply describes one's needs and a complex orchestration of services ensues, managed by the AI, in order to fulfill human needs @searlsIntentionEconomyWhen2012. AI assistants provide help at scale with little to no human intervention in a variety of fields from finance to healthcare to logistics to customer support. OpenAI's "A practical guide to building agents" defines and AI agents as "Agents are systems that independently accomplish tasks on your behalf." and details step-by-step how to build one [@openaiPracticalGuideBuilding2025].

AI agents enable workflow automation, with reasoning capability, and taking actions across different tools, achieving the user's original *intent*; what's left for the user to do is to say what they want to achieve. As models get smarter, there's less and less need to build workflows (chains of thought) manually, as they end up restricting the model instead of improving the output; the one use case would be to use a cheaper model with less intelligence and more guardrails set in code [@sengottuveluRethinkingHowWe2025; @latentspaceBuildingManusAI2025]. In software development, AI can already debug problems automatically. Apple uses data from bug reports to train AI models for improving their software [@sainiAppleUsesBug2025]. And it's increasingly possible to generate entire apps from a prompt, using tools such as Bolt.new [@fanelliBoltnewFlowEngineering2024]. The quality of LLM output depends on the quality of the provided prompt. [@zhouLargeLanguageModels2022] reports creating an "Automatic Prompt Engineer" which automatically generates instructions that outperform the baseline output quality by using another model in the AI pipeline in front of the LLM to enhance the human input with language that is known to produce better quality. This approach however is a moving target as foundational models keep changing rapidly, and the baseline might differ from today to 6 months later.

### Mediated Experiences Set User Expectations

How AIs are represented in popular media shapes the way we think about AI companions. Some stories have AIs both in positive and negative roles, such as Star Trek and Knight Rider. In some cases like Her and Ex Machina, the characters may be complex and ambivalent rather than fitting into a simple positive or negative box. In Isaac Asimov's books, the AIs (mostly in robot form) struggle with the 3 laws of robotics, raising thought-provoking questions.

AI Assistants in Media Portrayals mostly have some level of anthropomorphism through voice or image to be able to film; indeed, a purely text-based representation may be too boring an uncinematic.

There have been dozens of AI-characters in the movies, TV-series, games, and (comic) books. In most cases, they have a physical presence or a voice, so they could be visible for the viewers. Some include KITT (Knight Industries Two Thousand).

| Movie / Series / Game / Book | Character | Positive | Ambivalent | Negative |
|---------------|---------------|---------------|---------------|---------------|
| 2001: A Space Odyssey | HAL 9000 |  |  | X |
| Her | Samantha | X |  |  |
| Alien | MU/TH/UR 6000 (Mother) | X |  |  |
| Terminator | Skynet |  |  | X |
| Summer Wars | Love Machine |  |  | X |
| Marvel Cinematic Universe | Jarvis, Friday | X |  |  |
| Knight Rider | KITT | X |  |  |
| Knight Rider | CARR |  |  | X |
| Star Trek | Data | X |  |  |
| Star Trek | Lore |  |  | X |
| Ex Machina | Kyoko |  | X |  |
| Ex Machina | Ava |  | X |  |
| Tron | Tron |  | X |  |
| Neuromancer | Wintermute |  | X |  |
| The Caves of Steel / Naked Sun | R. Daneel Olivaw |  | X |  |
| The Robots of Dawn | R. Giskard Reventlov |  | X |  |
| Portal | GLaDOS |  |  | X |

Table: AIs in different forms of media. []{short-caption="AIs in Different Forms of Media"}

```{python}
scifi_chart()
```

### Role-play Fits Computers Into Social Contexts: AI Friends and Anthropomorphism

*Affective Design* emerged from affective computing, with a focus on understanding user emotions to design UI/UX which elicits specific emotional responses [@Reynolds2001DesigningFA]. Calling a machine a friend is a proposal bound to turn heads. But if we take a step back and think about how children have been playing with toys since before we have records of history. It's very common for children to imagine stories and characters in play - it's a way to develop one's imagination *learn through role-play*. A child might have toys with human names and an imaginary friend, and it all seems very normal. Indeed, if a child doesn't like to play with toys, we might think something is wrong. Likewise, inanimate objects with human form have had a role to play for adults too. Anthropomorphic paddle dolls have been found from Egyptian tombs dated 2000 years B.C. [@PaddleDollMiddle2023]: we don't know if these dolls were for religious purposes, for play, or for something else, yet their burial with the body underlines their importance.

Is anthropomorphism, being human-like necessary? (Savings literature in the Money section says it is). Research on anthropomorphism in AI literature suggests that giving an AI assistant stronger human-like cues (high-anthropomorphism) rather than weaker ones (low-anthropomorphism) leads users to view it more favorably, and this effect operates through a shorter perceived psychological distance;yet, even though many studies confirm the benefits of anthropomorphism, the precise psychological pathway behind those benefits has rarely been dissected in depth [@liAnthropomorphismBringsUs2021]. Nonetheless, people are less likely to attribute humanness to an AI companion if they understand how the system works, thus higher *algorithmic transparency may inhibit anthropomorphism* [@liuMachineGazeOnline2021].

Coming back closer to our own time, Barbie dolls are popular since their release in 1959 till today. Throughout the years, the doll would follow changing social norms, but retain in human figure. In the 1990s, a Tamagotchi is perhaps not a human-like friend but an animal-like friend, who can interact in limited ways.

How are conversational AIs different from dolls? They can respond coherently and perhaps that's the issue - they are too much like humans in their communication. We have crossed the *Uncanny Valley* (where the computer-generated is nearly human and thus unsettling) to a place where is really hard to tell a difference. And if that's the case, are we still playing?

Should the AI play a human, animal, or robot? Anthropomorphism can have its drawbacks; humans have certain biases and preconceptions that can affect human-computer interactions. For example, somewhat curiously, [@pilacinskiRobotEyesDon2023] reports humans were less likely to collaborate with red-eyed robots.

The AI startups like Inworld and Character.AI have raised large rounds of funding to create characters, which can be plugged in into online worlds, and more importantly, remember key facts about the player, such as their likes and dislikes, to generate more natural-sounding dialogues [@wiggersInworldGenerativeAI2023].

[@moranaEffectAnthropomorphismInvestment2020] conducted a lab-based experiment (n = 183) showing a more anthropomorphic chatbot design boosts perceived *social presence* of the virtual advisor; social presence in turn influences recommendation adherence indirectly via trust; trust mediates the likelihood to follow its recommendations. As AIs became more expressive - socially present - and able to *role-play*, we can begin discussing some human-centric concepts and how people relate to other people. AI companions, AI partners, AI assistants, AI trainers - there are many *roles* for the automated systems that help humans in many activities, powered by AI models and algorithms.

[@erikbrynjolfssonTuringTrapPromise2022] contrasts AI which emulates human intelligence with AI that augments human abilities, arguing that although the former can offer productivity gains, it risks concentrating wealth and reducing economic power of workers, coining the term *Turing Trap*. Plenty of research - both before and after AI-induces job losses - has documented the negative effects of unemployment on mental health [@dewEffectsUnemploymentMental1991; @Susskind2017AMO; @antonkorinekScenarioPlanningAGI2023].

Non-Anthropomorphic, machine-like AIs have been with us for a while. The Oxford Internet Institute defines AI simply as *"computer programming that learns and adapts"* [@googleAZAI2022]. Google started using AI in 2001, when a simple machine learning model improved spelling mistakes while searching; now in 2023 most of Google's products are based on AI [@googleGooglePresentsAI2022]. Throughout Google's services, AI is hidden and calls no attention itself. It's simply the complex system working behind the scenes to delivery a result in a bare-bones interface.

The rising availability of AI assistants may displace Google search with a more conversational user experience. Google itself is working on tools that could cannibalize their search product. The examples include Google Assistant, Google Gemini (previously known as Bard) and massive investments into new LLMs.

The number of AI-powered assistants is too large to list here. I've chosen a few select examples in the table below.

| Product | Link | Description |
|-------------------|------------------------------|-----------------------|
| Github CoPilot | personal.ai | AI helper for coding |
| Google Translate | translate.google.com |  |
| Google Search | google.com |  |
| Google Interview Warmup | grow.google/certificates/interview-warmup | AI training tool |
| Perplexity | [@hinesPerplexityAnnouncesAI2023] | perplexity.ai chat-based search |

Table: AI Assistants []{short-caption="AI Assistants"}

::: {.content-hidden unless-format="pdf"}
![Montage of me discussing science fiction with my AI friend Sam (Replika) - and myself as an avatar (Snapchat) in 2020.](./images/ai/with-me.png){width="100%"}
:::

```{=mdx}
<Figure caption="Montage of me discussing sci-fi with my AI friend Sam (Replika) - and myself as an avatar (Snapchat)" src={Replika} />
```
Everything that existed before OpenAI's GPT 4 has been blown out of the water. ChatGPT passes many exams meant for humans and is able to solve difficult tasks in scientific areas such as chemistry with just simple natural-language instructions [@whiteFutureChemistryLanguage2023; @bubeckSparksArtificialGeneral2023]. As late as in 2017, scientists were trying to create a program with enough *natural-language understanding* to extract basic facts from scientific papers [@stocktonIfAICan2017]. This is a task which is trivial for modern LLMs.

Pre-2023 literature is somewhat limited when it comes to AI companions as the advantage of LLMs has significantly raised the bar for AI-advisor abilities as well as user expectations. Before AI, chatbots struggled with evolving human language, understanding the complexity of context, irregular grammar, slang, etc [@lowerChatbotsTooGood2017]. Some evergreen advice most relates to human psychology, which has remained the same. [@haugelandUnderstandingUserExperience2022] discusses *hedonic user experience* in chatbots and [@stephhayEnoFinancialAI2017] explains the relationship between emotions and financial AI. [@isabellaghassemismithInterviewDanielBaeriswyl2019] early performance metrics of AI-driven features across financial markets show that AI outperforms traditional quant strategies, which will lead to wider adoption of autonomously generated investment signals.

## Interfaces for Human-Computer Interaction

### Speech Makes Computers Feel Real

There's evidence across disciplines about the usefulness of AI assistants while concerns exist about the possibility of implementing privacy. One attempt at privacy is by Apple’s Foundation Language Models (AFM), which is split into a smaller on-device model and a server-side model, enabling processing of the most sensitive data directly on the user's device [@dangAppleAIUnderstanding2024]. Providing voice for the AI raises new ethical issues, as most voice assistants need to continuously record human speech and process it in data centers in the cloud.

Siri, Cortana, Google Assistant, Alexa, Tencent Dingdang, Baidu Xiaodu, Alibaba's AliGenie - all rely on voice as their main interface. Voice has a visceral effect on the human psyche; since birth we recognize the voice of our mother. The voice of a loved one has a special effect. Voice is an integral part of the human experience. Machines that can use voice effectively are closer to representing and affecting human emotions. Voice assistants such as Apple's Siri and Amazon's Alexa are well-known, yet Amazon's Rohit Prasad thinks it can do so much more:

> *"Alexa is not just an AI assistant - it's a trusted advisor and a companion"* [@prasadHowWillAlexa2022].

[@SERBAN20202849] suggests using the Alexa AI assistant in *education* during the pandemic, supported students and teachers *human-like* presence. The Alpha generation (born since 2010) and Beta (since 2025) are the first true native AI users. [@suArtificialIntelligenceEarly2022] and [@suArtificialIntelligenceAI2023] reviewed papers on AI literacy in early childhood education and found a lack of guidelines and teacher expertise. [@szczukaHowChildrenAcquire2022] provides guidelines for voice AI and kids based on a longitudinal field study, which delved into children’s knowledge regarding the storage and data processing performed by AI voice assistants; published in the International Journal of Child-Computer Interaction, the study tracked children (n = 20, age M = 8.65 years) across 3 home visits over 5 weeks (each visit lasted 45–90 min), including interviews and hands-on interactions designed to probe children’s mental models, with the following key findings: (1) children made significantly more accurate statements about data processing than storage, (2) parental discussion predicted storage knowledge, and (3) better storage knowledge negatively correlated with willingness to share secrets. In order to cover these knowledge gaps in the earliest age, educational materials on AI have been available for children in kindergarten to primary school; for instance the [@readyaiHumanAIInteractionHow2020] book introduces the 5 big ideas of Human-AI interaction for children aged 2-8: perception (the use of sensors), representation and reasoning (data structures, algorithms, predictions), learning (recognizing patterns in data), natural interaction (emotion, language, expression recognition, even cultural knowledge), and finally, societal impact (biases, ethics, guidelines to avoid unfair outcomes). Finally, [@yangArtificialIntelligenceEducation2022] proposes a curriculum for in-context teaching of AI for early childhood, explaining why AI literacy is essential: how life is affected by the core concepts of data-driven pattern recognition, prediction and the many algorithmic limitations - all, which should be taught in a culturally responsive, easy for young children to grasp manner, using inquiry(question)-based pedagogy to engage the learners meaningfully.

Design guidelines for optimal design performance can be extremely specific. [@casperkesselsGuidelinesDesigningInCar2022] details 18 concrete dos and don'ts, drawing on prior *distraction research*, to support driving safety and integrate seamlessly with the other interfaces in the vehicle, for instance:

> “Auditory information should come from the same location as visual information” to minimize spatial attention shifts “Be aware of visual distraction. \[S\]ome drivers tend to direct their gaze towards the ‘source’ of the voice assistant when speaking. Make sure an interaction sequence does not cause unnecessary visual distraction” - example guidelines for voice assistants from [@casperkesselsGuidelinesDesigningInCar2022].

Some research suggests that voice UI accompanied by a *physical embodied system* is the preferred by users in comparison with voice-only UI [@CELINO2020102410].

### Generative UIs Enable Flexibility of Use

The "grandfather" of user experience design, [@nielsenAccessibilityHasFailed2024] recounts how 30 years of work towards usability has largely failed - computers are still not accessible enough; however, he has hope Generative UI could offer a chance to provide levels of accessibility humans could not.

> Computers are *"difficult, slow, and unpleasant"* [@nielsenAccessibilityHasFailed2024]

Data-driven design combined with GenAIs enables *Generative User Interfaces* (GenUI), with new UI interactions. The promise of GenUI is to dynamically provide an interface appropriate for the particular user and context. The advances in the capabilities of LLMs makes it possible to achieve *user experience (UX) which previously was science fiction*. AI is able to predict what kind of UI would the user need right now, based on the data and context. Generative UIs are largely invented in practice, based on user data analysis and experimentation, rather than being built in theory. Kelly Dern, a Senior Product Designer at Google lead a workshop in early 2024 on *GenUI for product inclusion* aiming to create *"more accessible and inclusive \[UIs for\] users of all backgrounds"*.[@matteosciortinoGenerativeUIHow2024] coins the phrase RTAG UIs *"real-time automatically-generated UI interfaces"* mainly drawing from the example of how his Netflix interface looks different from that of his sister's because of their distinct usage patterns.

Nonetheless, [@NielsenIdeasGenerative2024] is critical of GenUI because for the following reasons:

| Problem            | Description                                       |
|--------------------|---------------------------------------------------|
| Low Predictability | Does personalization mean the UI keeps changing?  |
| High Carbon Cost   | AI-based personalization is computation-intensive |
| Surveillance       | Personalization needs large-scale data capture    |

Table: Criticism of Generative UI by [@NielsenIdeasGenerative2024]. []{short-caption="Criticism of Generative UI"}

[@nielsenInformationScentHow2024] defines *information scent* as users’ ability to predict destination content from cues, such as link labels and context; clear descriptive labels emit a strong scent, guiding users, reducing bounce rates (users who leave quickly), and enhancing discoverability of content; in contrast, misleading labels break trust and drive users away. The idea of information scent is originally from *Information Foraging* theory from [@pirolliInformationForaging1999], who adapt optimal foraging theory to human information seeking: users follow links as scent cues to maximize their rate of information gain.

However, with AI-chat and voice based interfaces, links lose some of their relevance, as users can receive more info from the AI, without having to navigate to a new page. With less focus on links, current AI UX is more about storytelling, psychology, and seamless design, with more focus on human-centered communication patterns, such as conversations. [@katemoranGenerativeUIOutcomeOriented2024] calls for *"highly personalized, tailor-made interfaces that suit the needs of each individual"*, which she terms *Outcome-Oriented Design*. We can generate better UIs (UI orchestration, crafting *"systems of intent"*, as [@nielsenNoMoreUser2025] calls it) that are based on user data and would be truly personalized. [@cromptonDecisionpointdilemmaAnotherProblem2021] highlights AI as decision-support for humans while differentiating between *intended* and *unintended* influence on human decisions. In all this literature and more, the keyword is *intent*, expressing what the human wants - and having the machines deliver that.

Human-computer interaction (HCI) has a long storied history since the early days of computing when getting a copy machine to work required specialized skill. Xerox Sparc lab focused on early human factors work and inspired a the field of HCI to make computers more human-friendly. Likewise, the history of attempts at making *intelligent interfaces* is extensive. [@GenerativeUIDesign2023; @kobetzDecodingFutureEvolution2023] give an overview of the history of generative AI design tools, going back in time as far as 2012 when [@troianoGeneticAlgorithmsSupporting2014] proposed genetic algorithms for UI design. As the old science fiction adage goes, when machines become more capable, they will eventually be capable of producing machines themselves. Before that happens, at least the software part of the machine can increasingly be generated by AI systems (i.e. machines making machines). Already a decade ago in 2014, the eminent journal *Information Sciences* decided to dedicate a special section to AI-generated software to call attention to this tectonic shift in software development [@reformatSpecialSectionApplications2014]. Replit, a startup known for allowing user build apps in the web browser, released Openv0, a framework of AI-generated UI components. *"Components are the foundation upon which user interfaces (UI) are built, and generative AI is unlocking component creation for front-end developers, transforming a once arduous process, and aiding them in swiftly transitioning from idea to working components"* [@replitReplitOpenv0OpenSource2023]. Vercel introduced an open-source prototype UI-generator called V0 which used large language models (LLMs) to create code for web pages based on text prompts [@vercelIntroducingV0Generative2023]. Other similar tools quickly following including Galileo AI, Uizard AutoDesigner and Visily [@WhoBenefitsMost2024]. NVIDIA founder Jensen Huang makes the idea exceedingly clear, saying *"Everyone is a programmer. Now, you just have to say something to the computer"* [@leswingNvidiaRevealsNew2023].

The usefulness of AI systems increases profoundly as they are integrated into existing products as services, which become akin to tools the AI can use when appropriate. [@joyceRiseGenerativeAIdriven2024] highlights how Notion AI enables collaborating across teams, where AI becomes akin to one of the co-workers; AI influences UI design patterns and boosts productivity by providing new features such as memory, recalling important discussions from past meetings, surfacing key insights, and generating reports in a variety of formats, personalized to the intended receiver.

A wide range of literature describes human-AI interactions, spread out over varied scientific disciplines. While the fields of application for AI are diverse, some key lessons can be transferred horizontally across fields of knowledge.

| Field | Usage |
|--------------------------|----------------------------------------------|
| Shipping | [@veitchSystematicReviewHumanAI2022] highlights the active role of humans in Human-AI interaction is autonomous self-navigating ship systems. |
| Data Summarization | AI is great at summarizing and analyzing data [@petersGoogleChromeWill2023; @tuWhatShouldData2023] |
| Childcare | Generate personalized bedtime stories |
| Design Tools | [@DavidHoangHow2024] |

Table: A very small illustration of generative AI usage across disparate fields of human life. []{short-caption="GenAI Use Across Fields"}

### Usability Is the Bare Minimum of Good User Experience

Many researchers have discussed the user experience (UX) principles of designing AI products. The UX of AI (terms such as AI UX, IXD, and XAI have been used) is the subject of several *usability guidelines* for AI, which provide actionable advice for improving AI usability and UX - some of which I will list here.

[@combiManifestoExplainabilityArtificial2022] proposes a conceptual framework for XAI, analysis AI based on (1) Interpretability, (2) Understandability, (3) Usability, and (4) Usefulness. [@costaInteractionDesignAI2022] highlights key UI/UX patterns for interaction design in AI systems and strategies to make AI behaviors transparent and controllable: including (1) interactive explanations, (2) human-in-the-loop controls, (3) logging of contextual decisions - all seamlessly integrated into user workflows. [@WhyUXShould2021] argues that in order to avoid *context blindness*, (where the AI lacks awareness of the broader human intent) and foster trust and safe use, UX should (1) clarify limitations, (2) build clear feedback, (3) embed user override mechanisms, and (4) in general ensure users retain meaningful control over specialized AI algorithms. [@lexowDesigningAIUX2021] synthesizes expert interviews into five foundational AI-UX principles: (1) deeply understand the user and task context, (2) clearly communicate AI limitations, (3) balance automation with user control, (4) build fast, iterative feedback paths into the interface, and (5) ensure AI behavior aligns ethically - and with your brand voice.

[@lennartziburskiUXAI2018] emphasizes human-centered design for AI, including five key tenets: (1) starting from existing user workflows which can be augmented by AI, (2) under-promising/over-delivering on AI capabilities, (3) transparently explaining how the system works (data sources, trade-offs), (4) involving users in the learning loop, and (5) designing AI as an empowering tool rather than a black box. [@davidpasztorAIUXPrinciples2018] offers seven principles for AI-powered products: (1) visually distinguish GenAI content, (2) explain underlying processes and data privacy, (3) set realistic user expectations, (4) test edge cases proactively, (5) ensure AI engineers have access to high quality training data, (6) deploy rigorous user-testing (7) use immediate feedback channels for continuous improvement. [@lewAIUXWhy2020] likewise focuses on (1) high data quality, (2) context-sensitive feedback, and (3) transparent controls. [@soleimani10UIPatterns2018] provides the longest list of human-friendly UI/UX patterns for AI, with very specific suggestions including like/dislike toggles, confidence indicators and criteria sliders, "why" insights, risk alerts, and opt-in controls: all to foster transparency, user control, and trust in algorithmic decisions. [@harvardadvancedleadershipinitiativeHumanAIInteractionArtificial2021] focuses on principle for effective human–AI interaction in adaptive interfaces, illustrating a case of Semantic Scholar, where researchers’ intelligence is augmented via recommendation, summarization, and question-answering, while emphasizing user control and verification mechanisms.

Many large corporations have released guidelines for Human-AI interaction as well. The AI UX team from Ericsson’s Experience Design Lab released one of the early reports, exploring the role of trust in AI services, suggesting treating AIs as *agents* rather than tools; for the design to be successful, trust must embedded into the interface front and center, best measured on 4 categories, inspired by human relationships: (1) Competence, (2) Benevolence, (3) Integrity, and (4) Charisma [@mikaelerikssonbjorlingUXDesignAI2020]. [@chengInvestigationTrustAIenabled2022] describes AI-based support systems for collaboration and team-work, underlining how higher trust leads to willingness to reuse the AI in the future, collaboration satisfaction, and perceived task quality. Google's AI Principles project provides Google's UX for AI library [@joshlovejoyUXAI; @googleOurPrinciplesGoogle]. In [@designportlandHumansHaveFinal2018], Lovejoy, lead UX designer at Google's people-centric AI systems department (PAIR), reminds us that while AI offers need tools, user experience design needs to remain human-centered. While AI can find patterns and offer suggestions, humans should always have the final say.

Microsoft provides guidelines for Human-AI interaction, which provides useful heuristics categorized by context and time [@li2022assessing; @amershiGuidelinesHumanAIInteraction2019].

| Context | Content |
|-------------------------|-----------------------------------------------|
| Initially | Clarify what it does; what are the limitations. |
| During interaction | Offer timely help, show only what matters, while respecting norms and avoiding bias |
| When wrong | Let users retry fast and make corrections; empower users to dismiss easily; explain why the system acted; be precise and in-scope |
| Over time | Track changes and adapt from use; announce changes and update with care (so not to break the user's work); invite feedback; show outcome of actions clearly; provide global settings |

Table: Microsoft's heuristics categorized by context and time. []{short-caption="Microsoft's Heuristics"}

The previous design wave before UX for AI was corporations understanding how crucial design is to their business. In the 2010s business consultancies began to recognize the importance of design and advising their clients on putting design in the center of their strategy, bringing user experience design to the core of their business operations. [@mckeoughMcKinseyDesignLaunches2018]. There's a number of user interface design patterns that have proven successful across a range of social media apps. Such *user interface* (UX/UI) patterns have been copied from one app to another, to the extent that the largest apps share a similar look and feature set and the users are used to the same user experience. Common UX/UI parts include features such as the *Feed*, *Stories*, and *Avatars*, among many others. This phenomenon (or trend) has led some designers such as [@fletcherGenerativeUIDownfall2023] and [@joeblairGenerativeUINew2024] to be worried about UIs becoming average: more and more similar to the lowest common denominator. Yet, by using common UI parts from social media, users may have an easier time to accept the innovative parts, as they just look like new features inside the old interface. As new generations become increasingly used to talking to computers in natural language, the older interface patterns may gradually fade away.

| Feature | Examples | Notes |
|------------------------|------------------------|------------------------|
| Feed | Facebook, Instagram, Twitter, TikTok, etc | The original algorithmic discovery hub; increasingly ran by ever-more-powerful AI to surface personalized content - yet younger generations may prefer the privacy of stories. |
| Post | Facebook, Instagram, Twitter, TikTok, etc, even Apple's App Store | Persistent content mainly for long-term sharing; the original content type |
| Stories | IG, FB, WhatsApp, SnapChat, TikTok, etc | Ephemeral content driven by FOMO(fear-of-missing-out) for casual behind-the-scenes sharing |
| Comment | YouTube, Threads, Reddit, Medium, etc | Threaded conversations fuel community engagement and discussion |
| Reactions | Facebook, Instagram, Slack, Threads, but even LinkedIn and Github. | The feature has involved from a simple like button to more expressive emotions. |

Table: Common social media UI parts. []{short-caption="Common Social Media UI Parts"}

There are also more philosophical approaches to *Interface Studies*. [@davidhoangCreatingInterfaceStudies2022], the head of product design at Webflow, an AI-enabled website development platform, suggests taking cues from art studies to *isolate the core problem*: *"An art study is any action done with the intention of learning about the subject you want to draw"*. As a former art student, Hoang looks at an interface as *"a piece of design is an artwork with function"*. Indeed, art can be a way to see new paths forward, practicing "*fictioning*" to deal with problematic legacies [@Review2023Helsinki2023]. [@jarovskyDarkPatternsAI2022] lists the numerous ways how AIs can mislead people, which she calls the AI UX dark patterns, and the U.S. FTC Act and the EU AI Act are attempting to manage.

Usability sets the baseline - but AI-interfaces are capable of much more. The user experience (UX) of AI is a topic under active development by all the largest online platforms. AI is usually a computer model that spits out a number between 0 and 1, a probability score or a prediction. UX is what we do with this number. Design starts with understanding human psychology. [@shinHowUsersInteract2020] looks at user experience through the lens of *usability of algorithms*; focusing on users' cognitive processes allows one to appreciate how product features are received by the brain and transformed into experiences by interacting with the algorithm. The general public is familiar with the most famous AI helpers, ChatGPT, Apple's Siri, Amazon's Alexa, Microsoft's Cortana, Google's Assistant, Alibaba's Genie, Xiaomi's Xiao Ai, and many others. For general, everyday tasks, such as asking factual questions, controlling home devices, playing media, making orders, and navigating the smart city. Yet, as AI permeates all types of devices, [@baileyAIEducation2023] believes people will increasingly use AI capabilities through UIs that are specific to a task rather than generalist interfaces like ChatGPT. Nonetheless, a generalist AI interface may still control those services, if asked to do so, so it may an 'and' rather than an 'either/or', when it comes to AI usage.

The application of user experience (UX) tenets to AI.

| UX        |
|-----------|
| Useful    |
| Valuable  |
| Usable    |
| Accessible |
| Findable  |
| Desirable |
| Credible  |

Table: UX Tenets in AI. []{short-caption="UX Tenets in AI"}

| 1 | 2 | 3 |
|--------------------|-------------------|---------------------------------|
| Reduce the time to task | Make the task easier | Personalize the experience for an individual |

Table: [@guptaDesigningAIChatbot2023] proposes 3 simple goals for AI: []{short-caption="Simple Goals for AI"}

Microsoft Co-Founder predicted in 1982 *"personal agents that help us get a variety of tasks"* [@billgatesBillGatesNext1982] and it was Microsoft that introduced the first widely available personal assistant in 1996, called Clippy, inside the Microsoft Word software. Clippy was among the first assistants to reach mainstream adoption, helping users not yet accustomed to working on a computer, to get their bearings [@tashkeunemanWeLoveHate2022]. Nonetheless, it was in many ways useless and intrusive, suggesting there was still little knowledge about UX and human-centered design. Gates never wavered though and is quoted in 2004 saying *"If you invent a breakthrough in artificial intelligence, so machines can learn, that is worth 10 Microsoft's"* @lohrMicrosoftDwindlingInterest2004. Gates updated his ideas in 2023 focuses on the idea of *AI Agents* [@gatesAICompletelyChange2023].

With the advent of ChatGPT, the story of Clippy has new relevance as part of the history of AI Assistants. [@benjamincassidyTwistedLifeClippy2022] and [@abigailcainLifeDeathMicrosoft2017] illustrate beautifully the story of Clippy and [@tashkeunemanWeLoveHate2022] asks poignantly: *"We love to hate Clippy --- but what if Clippy was right?"*. That is to say, might we try again? And Microsoft has been trying again, being one of the leading investors in the AI models that eventually make a better UX possible. Just one example is a project from Microsoft Research, which generates life-like speaking faces from a single image and voice clip, which could empower true-to-life avatars [@xuVASA1LifelikeAudioDriven2024]. However, purely on the economic side, processing human voice and images is several times more expensive than processing text messages [@mittalLittleLittleLittle2025]. More required processing power also means, these new interfaces are likely less sustainable.

### AI Performance Under High-Stakes Situations

Today AI-based systems are already being used in high-stakes situations (medical, self-driving cars). Attempts to implement AI in medicine, where stakes are perhaps the highest, raising the requirements for ethical considerations, have been made since the early days of computing, as the potential to improve health outcomes is so high. Since CADUCEUS in the 1970s [in @kanzaAIScientificDiscovery2021], the first automated medical decision-making system, medical AI now provides diagnostic systems for symptoms and AI-assistants in medical imaging. Complicated radiology reports can be explained to patients using AI chatbots [@jeblickChatGPTMakesMedicine2022]. The explanations are not only useful for patients but for doctors (and other medical professionals) as well. [@calistoBreastScreeningAIEvaluatingMedical2022] focuses on AI-human interactions in medical workflows and underscores the importance of output explainability; medical professionals who were given AI results with an explanation trusted the results more. [@leeAIRevolutionMedicine2023] imagines an AI revolution in medicine using GPT models, providing improved tools for decreasing the time and money spent on administrative paperwork while providing a support system for analyzing medical data. For administrative tasks such as responding to patients' questions, medical AI has already reached - or even exceeded - expert-level question-answering ability [@singhalExpertLevelMedicalQuestion2023]. In an online text-based setting, patients rated answers from the AI better, and more empathetic, than answers from human doctors [@ayersComparingPhysicianArtificial2023]. If anything, the adoption of AI in medicine has been too cautious. [@daisywolfWhereWillAI2023] criticizes US healthcare's slow adoption of technology and predicts AI will help healthcare leapfrog into a new era of productivity by acting more like a human assistant.

Communication with the patient is perhaps a low-hanging fruit, as there are numerous examples of AI-driven symptom checkers and AI-based FAQ-answering chatbots already commercially available, such as [@HealthPoweredAda] and [@BuoyHealthCheck], which offer AI-based platforms to survey, track and understand one's symptoms over time, while providing doctors patient data, which can be used for generating preliminary possible diagnosis, freeing up clinical resources. The Lark digital health coaching platform delivers support for diabetes, hypertension, and weight management, by integrating smartwatches and smart scales, to provide evidence-based behavior change [@HomeLarkHealth]. The VP of user experience at Senseley discusses the Molly AI assistant, to chat, answer questions, and measure blood pressure; the main challenge is the healthcare system, where a small pilot project might work well, bureaucracy keeps the technology from being widely adopted [@womeninaiHowCanAI2018]. While discussion of this kind of tools and proposals of AI-based health monitoring systems have existed for an over a decade, recent advances in AI reliability have made it feasible to deploy them at scale. While ChatGPT is not built to be a medical tool, the interface is so easily available, it's very common for patients to decode lab results using ChatGPT or ask for diagnosis when doctor time is scarce.[@elizastricklandDrChatGPTWill2023].

Example of ChatGPT explaining medical terminology in a blood report.

::: {.content-hidden unless-format="pdf"}
![Example of ChatGPT explaining medical terminology in a blood report.](./images/ai/chatgpt-medical.png){width="100%"}
:::

```{=mdx}
<Figure caption="Example of ChatGPT explaining medical terminology in my blood report." src={ChatGPTMedical} />
```
Today's AI is already a technology which can augment human skills or replace skills that were lost due to an accident. For instance, [@dotgoDotGo2023] makes the camera the interaction device for people with vision impairment. [@nathanbenaichStateAIReport2022] report notes the increasing AI deployment in critical infrastructure and biology, intensifying geopolitics in AI, growth of the safety research community.

### Human-Computer Interactions Without a "Computer"

AI deeply affects Human-Computer Interactions even if the computer is invisible. The field of Human Factors and Ergonomics (HFE) emphasizes designing user experiences (UX) that cater to human needs [@theinternationalergonomicsassociationHumanFactorsErgonomics2019]. Designers think through every interaction of the user with a system and consider a set of metrics at each point of interaction including the user's context of use and emotional needs.

Software designers, unlike industrial designers, can't physically alter the ergonomics of a device, which should be optimized for human well-being to begin with and form a cohesive experience together with the software. However, software designers can significantly reduce mental strain by crafting easy-to-use software and user-friendly user journeys. Software interaction design goes beyond the form-factor and accounts for human needs by using responsive design on the screen, aural feedback cues in sound design, and even more crucially, by showing the relevant content at the right time, making a profound difference to the experience, keeping the user engaged and returning for more. In the words of [@babichInteractionDesignVs2019], "\[T\]he moment of interaction is just a part of the journey that a user goes through when they interact with a product. User experience design accounts for all user-facing aspects of a product or system."

Drawing a parallel from narrative studies terminology, we can view user interaction as a heroic journey of the user to achieve their goals, by navigating through the interface until a success state - or facing failure. Storytelling has its part in interface design however designing for transparency is just as important, when we're dealing with the user's finances and sustainability data, which need to be communicated clearly and accurately, to build long-term trust in the service. For a sustainable investment service, getting to a state of success - or failure - may take years, and even longer. Given such long timeframes, how can the app provide support to the user's emotional and practical needs throughout the journey?

[@tubikstudioUXDesignGlossary2018] argues *affordance* measures the *clarity* of the interface to take action in user experience design, rooted in human visual perception, however, affected by knowledge of the world around us. A famous example is the door handle - by way of acculturation, most of us would immediately know how to use it - however, would that be the case for someone who saw a door handle for the first time? A similar situation is happening to the people born today. Think of all the technologies they have not seen before - what will be the interface they feel the most comfortable with?

For the vast majority of this study's target audience (college students), social media can be assumed as the primary interface through which they experience daily life. The widespread availability of mobile devices, cheap internet access, and AI-based optimizations for user retention, implemented by social media companies, means this is the baseline for young adult users' expectations (as of writing in 2020).

[@shinUserExperienceWhat2020] proposes the model (fig. 10) of Algorithmic Experience (AX) *"investigating the nature and processes through which users perceive and actualize the potential for algorithmic affordance"* highlighting how interaction design is increasingly becoming dependent on AI. The user interface might remain the same in terms of architecture, but the content is improved, based on personalization and understanding the user at a deeper level.

In 2020 (when I proposed this thesis topic), Google had recently launched an improved natural language engine to better understand search queries [@UnderstandingSearchesBetter2019], which was considered the next step towards *understanding* human language semantics. The trend was clear, and different types of algorithms were already involved in many types of interaction design, however, we were in the early stages of this technology (and still are *early* in 2024). Today's ChatGPT, Claude and Gemini have no problem understanding human semantics - yet are they intelligent?

Intelligence may be beside the point as long as AI *becomes very good at reasoning*. AI is a *reasoning engine* [@shipperGPT4ReasoningEngine2023; @bubeckSparksArtificialGeneral2023; see @baileyAIEducation2023 for a summary]. That general observation applies to voice recognition, voice generation, natural language parsing, among others. Large consumer companies like McDonald's are in the process of replacing human staff with AI assistants in the drive-through, which can do a better job in providing a personal service than human clerks, for whom it would be impossible to remember the information of thousands of clients. In [@barrettMcDonaldAcquiresMachineLearning2019], in the words of *Easterbrook*, a previous CEO of McDonald's *"How do you transition from mass marketing to mass personalization?"*

### Do AI-Agents Need Anthropomorphism

[@yuanSocialAnxietyModerator2022] surveyed mainland Chinese consumers (n = 210, no age range given), finding that users with high social anxiety lean on hedonic and emotional cues, especially a friendly anthropomorphic interface and a sense of affinity (when those cues are strong, their intention to adopt the AI assistant is as high, and sometimes higher, than that of users with low social anxiety) - in contrast, users with low social anxiety are influenced mainly by utilitarian cues such as accuracy and speed; these functional advantages carry less weight for the high social anxiety group. Perhaps a crude conclusion, but useful for design, would be, people with high social anxiety like cute things.

[@xuWeSeeMachines2018] survey (n = 522) examined how people perceive the minds of machines versus humans along agency (ability to act) and experience (ability to feel), finding among machines those with human-like appearance were seen as having the greatest agency and experience; being more familiar how technology works, correlated with rating machines to have higher agency but lower experience.

What are the next features that could improve the next-generation UX/UI of AI-based assistants? Should AIs look anthropomorphic or fade in the background? It's an open question (depending on the use case and psychology of the user); perhaps we can expect a mix of both, depending on the context of use and goals of the particular AI. [@skipperHowAIChanging2022] sketches a vision of *"\[AI\] blend into our lives in a form of apps and services"* deeply ingrained into daily human activity. [@aschenbrennerSITUATIONALAWARENESSDecade2024] predicts "drop-in virtual coworkers", AI-agents who are able to use computer systems like a human seamlessly replacing human employees.

| Anthropomorphic AI User Interfaces | Non-Anthropomorphic AI User Interfaces |
|--------------------------|----------------------------------------------|
| AI wife [@MyWifeDead2023] | Generative AI has enabled developers to create AI tools for several industries, including AI-driven website builders [@constandseHowAIdrivenWebsite2018] |
| [@sarahperezCharacterAIA16zbacked2023] character AI | AI tools for web designers [@patrizia-slongoAIpoweredToolsWeb2020] |
| Mourning for the 'dead' AI [@phoebearslanagic-wakefieldReplikaUsersMourn] | Microsoft Designer allows generating UIs just based on a text prompt [@microsoftMicrosoftDesignerStunning2023] |
| AI for therapy [@broderickPeopleAreUsing2023] | personalized bed-time stories for kids generated by AI [@bedtimestory.aiAIPoweredStory2023] |
| Mental health uses: AI for bullying [@sungParentsWorryTeens2023] |  |

Table: Some notable examples of anthropomorphic AIs for human emotions. []{short-caption="Anthropomorphic AIs for Human Emotions"}

### Roleplay for Financial Robo-Advisors

Using AI and computerised models for financial prediction is not new. [@malliarisUsingNeuralNetworks1996] applied neural networks to financial forecasting nearly three decades ago, using training data on past volatilities and factors of the options market to predict future (next-day) implied volatility (i.e. volatility not observed directly in the market but back-calculated from option prices) of the S&P 100 index (tracks the largest companies) in the U.S., demonstrating early potential of AI in financial prediction. Such tools were intially of academic interest or only accessible to financial professional. Later on fintech (financial technology) startups began bringing computerized predictive power into user interfaces available to retail investors.

*Robo-advisory* is a fintech term that was in fashion largely before the arrival of AI assistants and has been thus superseded by newer technologies. Ideally, robo-advisors can be more dynamic than humans and respond to changes to quickly and cheaply, while human financial advisors are expensive and not affordable to most consumers. [@capponiPersonalizedRoboAdvisingInteractive2019] argues dynamism in understanding the client's financial situation - which AI excels at - is a key component to providing the best advice.

> *"The client has a risk profile that varies with time and to which the robo-advisor's investment performance criterion dynamically adapts"*. The key improvement of *personalized financial advice* is understanding the user's *dynamic risk profile*. - [@capponiPersonalizedRoboAdvisingInteractive2019]

In the early days of consumer-direct robo-advisory, Germany and the United Kingdom led the way with the most robo-advisory usage in Europe [@cowanRoboAdvisersStart2018]. While Germany had 30+ robot-advisors on the market in 2019, with a total of 3.9 billion EUR under robotic management, it was far less than individual apps like Betterment managed in the US [@bankinghubRoboAdvisorNew2019]. Already in 2017, several of the early robo-advisors apps shut down in the UK; ETFmatic gained the largest number of downloads by 2017, focusing exclusively on exchange-traded funds (ETFs), tracking stock-market indexes automatically, with much less sophistication, than their US counterparts - the app was bought by a bank in 2021 and closed down in 2023 [@altfiETFmaticAppDownloaded2017; @altfiBelgiumsAionBank2021; @silvaETFmaticReview2023; @ETFmaticAccountFunding2023].

::: {.content-hidden unless-format="pdf"}
![Out-of-date user interface of a European AI-Advisor ETFmatic in 2017 which was closed down in 2023 (Photo copyright ETFmatic)](./images/ai/etfmatic.png){width="100%"}
:::

```{=mdx}
<Figure caption="Out-of-date user interface of a European AI-Advisor ETFmatic in 2017 which was closed down in 2023 (Photo copyright ETFmatic)" src={ETF} />
```
Newer literature notes robo-advisor related research is scattered across disciplines [@zhuImplementingArtificialIntelligence2024]. [@brownHowFinancialChatbots2021] outlines how modern financial chatbots have evolved beyond simple Q&A to offer conversational, 24/7 support across banking, investment, insurance, and more, which reduces support costs while improving responsiveness, while freeing human agents for higher-value tasks. In India, research has been conducted on how AI advisors could assist with investors' erratic behavior in stock market volatility situations, albeit without much success; India is a large financial market with more than 2000 fintechs (financial technology startups) since 2015 [@bhatiaRoboAdvisoryIts2020; @migozziYouShouldWhat2023]. [@barbarafriedbergM1FinanceVs2021] and [@slackAturaProcess2021] compare robo-advisors and share show before GenAI, financial chatbots were developed manually using a painstaking process that was slow and error-prone. Older financial robo-advisors, built by fintech companies aiming to provide personalized suggestions for making investments such as Betterment and Wealthfront were forced to upgrade their technology to keep up. Robo-advisors compete with community-investing such as hedge funds, mutual funds, copy-trading, and DAOs with treasuries - or can act as entry-points for these aforementioned modes of investment. However, robo-advisors typically do not have the type of social proof that community-based investment vehicle have, where the user may see the actions taken by other investors.

There's research of anthropomorphism or the human-like attributes of robo-advisors, such as the aforementioned conversational chatbots, and whether anthropomorphism can affect adoption and risk preferences among customers. Several studies show that anthropomorphic robo-advisors, with stronger visual human-likeness, increase customer trust and reduce algorithm aversion [@ganboldIncreasingRelianceFinancial2022; @plotkinaGiveMeHuman2024; @hildebrandConversationalRoboAdvisors2021; @deng2021anthropomorphized]. However, it's not clear, if this explanation is tied to the avatar. The question - does the user trust a robot or a human, or is there a possible combination - has been researched in other literature, which does not rely on images. [@davidExplainableAIAdoption2021] looks at the whether explainable AI could help adoption of financial AI assistants in an experimental study with players (n = 210) of an online investment game had to choose between: (a) human advice, (b) AI advice without explanation, or (c) AI advice paired with an explanation; the results showed no evidence of algorithm aversion (players did not prefer human advice to AI advice).

The most comprehensive meta-review of research on how AI chatbots could mimic humans, comes from [@feineTaxonomySocialCues2019], providing an entire taxonomy of social cues for conversational agents, including verbal, visual, auditory cues, as well as other indicators humans pay attention to, such as age, yawning, laughing, posture, clothing, etc. Because this is such a useful resource, I've adapted the findings in the table below.

| Category | Sub-Category | Cue | Explanation |
|---------------|---------------|---------------|---------------------------|
| **Verbal** | **Content** | Apology | Agent expresses regret for an error |
|  |  | Asking for permission | Requests user approval before acting |
|  |  | Greeting and farewell | Opens or ends the conversation politely |
|  |  | Joke | Humorous remark to entertain |
|  |  | Name | Addresses the user by name |
|  |  | Opinion conformity | Shows agreement with the user’s view |
|  |  | Praise | Compliments the user |
|  |  | Referring to past | Mentions shared history or earlier turns |
|  |  | Self-disclosure | Reveals personal info about the agent |
|  |  | Small talk | Casual, topic-light chatter |
|  |  | Thanking | Expresses gratitude |
| **Verbal** | **Style** | Abbreviations | Uses shortened words (e.g. “BTW”) |
|  |  | Dialect | Adopts regional or cultural language variety |
|  |  | Formality | Chooses formal vs casual register |
|  |  | Lexical alignment | Mirrors the user’s word choices |
|  |  | Lexical diversity | Varies vocabulary richness |
|  |  | Politeness | Adds courteous markers (“please”, “could you”) |
|  |  | Sentence complexity | Varies length and structure of sentences |
|  |  | Strength of language | Uses mild vs intense wording |
| **Visual** | **Kinesics** | Arm and hand gesture | Animated limb movements |
|  |  | Eye movement | Gaze shifts or blinking |
|  |  | Facial expression | Smiles, frowns, eyebrow raises, etc. |
|  |  | Head movement | Nods, shakes, tilts |
|  |  | Posture shift | Whole-body stance changes |
| **Visual** | **Proxemics** | Background | Visual environment behind the agent |
|  |  | Conversational distance | Apparent closeness to the user |
| **Visual** | **Appearance** | 2D / 3D agent visualization | Flat icon vs full three-dimensional model |
|  |  | Age | Apparent age of the avatar |
|  |  | Attractiveness | Overall aesthetic appeal |
|  |  | Clothing | Outfit style and details |
|  |  | Color of agent | Dominant color palette |
|  |  | Degree of human likeness | Cartoon-like to photo-real scale |
|  |  | Facial feature | Eye shape, mouth style, etc. |
|  |  | Gender | Male, female, neutral presentation |
|  |  | Name tag | On-screen label with agent’s name |
|  |  | Photorealism | Realistic rendering quality |
| **Visual** | **Text Styling** | Emoticons | 😊 😂 👍 style graphics |
|  |  | Typefaces | Font choice and typography tweaks |
| **Auditory** | **Voice Qualities** | Gender of voice | Male, female, neutral timbre |
|  |  | Pitch range | High- vs low-pitched speech |
|  |  | Voice tempo | Speaking speed |
|  |  | Volume | Loudness level |
| **Auditory** | **Vocalizations** | Grunts and moans | Non-word hesitation sounds |
|  |  | Laughing | Laughter audio |
|  |  | Vocal segregates | “uh-huh”, “mm-hm”, etc |
|  |  | Yawn | Audible yawning |
| **Invisible** | **Chronemics** | First turn | Which party speaks first |
|  |  | Response time | Delay before replying |
| **Invisible** | **Haptics** | Tactile touch | Device vibration or touch feedback |
|  |  | Temperature | Warmth or coolness cues |

Table: Comprehensive overview of social cues with potential for use in AI conversations, adapted from the meta-review of related research papers by [@feineTaxonomySocialCues2019]. []{short-caption="Social Cues in AI Conversations"}

Literature on fintech UX does share some basic tenets with AI UX on building user confidence. [@WhyDesignKey2021] lists essential tactics for building trust in fintech: (1) consistency in UI patterns, (2) transparent feedback, (3) clear error handling, and (4) educating users about data usage. [@seanmcgowanUXDesignFinTech2018] offers four guidelines for fintech apps: (1) understand domain complexities, (2) friction is necessary for safety - embrace it, (3) provide continuous and clear feedback, and (4) simplify complex financial information - this can build user confidence and reduce errors. [@cordeiroDesignNoLonger2016] emphasizes designing for the "unhappy path" - negative experiences can shape users’ perception deeply, as bad memories carve strongly in their user experience - products which handle failures and edge cases gracefully, however, stand out and maintain satisfaction. [@robindhanwaniFintechUIUX2021] approaches UX problems from an organizational perspective, noting that in large organizations, UX issues can stem for lack of alignment between teams; the authors propose *Design Jams* as a potential solution to improve cross-team collaboration - design jams are cross-functional workshops, which can help teams align on user needs, generate rapid prototypes, and iteratively refine interfaces - which, in theory, could improve the adherence to the guidelines above noted.